"""
Python 的迭代器（Iterator）是这门语言“优雅”与“高效”的基石。
很多初学者只知道用 for 循环，却不知道背后的协议与状态机机制。
如果不理解它，你写的代码在处理大数据时就是一枚“内存炸弹”。
"""

# 第一步：核心概念与底层原理（The "Why" and "Kernel"）
"""
1. 痛点打击：为什么要发明“迭代器”？
场景： 假设你要处理一个 100GB 的日志文件，每一行都是一条用户记录，你需要从中找到包含 "ERROR" 的行。你的电脑只有 8GB 内存。
    没有迭代器（Eager Loading / 饿汉式）：
        你需要创建一个巨大的列表（List），把 100GB 的数据一次性全部读入内存。
        后果： 操作系统立刻报错 MemoryError，或者触发疯狂的 Swap（磁盘交换），导致电脑卡死。就像你试图把整个水库的水一口气喝干，结果肚子撑破了。
    有了迭代器（Lazy Evaluation / 惰性求值）：
        你不需要把所有数据都读进来。你只需要一个“水龙头”。
        机制： 你向系统申请一个文件句柄（这就好比一个迭代器）。当你需要处理第一行时，你拧一下水龙头（调用 next()），系统从磁盘读取一行数据给你。
            处理完这行，数据就可以从内存丢弃。然后再拧一下，出第二行。
        后果： 无论文件是 100GB 还是 10TB，你的内存占用始终只有 几KB（保存当前这一行的开销）。
生活类比：
    列表（List） 就像是自助餐盘子，你必须把所有想吃的菜一次性夹满盘子端回座位（占地方，拿多了吃不完浪费）。
    迭代器（Iterator） 就像是回转寿司的传送带。厨师（数据源）在不断做寿司，你（CPU）坐在那里，传送带转过来一个，你拿一个吃一个。
        你不需要关心后面还有多少个，也不需要把整个传送带搬回家。

2. 降维打击：底层是如何运行的？
    在 Python 解释器（CPython）和操作系统层面，迭代器本质上是一个 保存了状态的“游标”对象。
    协议层（The Protocol）：
        在 Python 中，任何实现了 __iter__() 和 __next__() 方法的对象都是迭代器。
        __iter__: 返回它自己（告诉解释器：我是个迭代器）。
        __next__: 返回下一个数据。如果没有数据了，抛出 StopIteration 异常（告诉循环：结束了，别拧了）。
    内存与 CPU 视角（核心差异）：
        让我们对比 list 和 iterator 在底层的巨大差异。
        List (数组结构):
            内存： 这是一个连续的（或引用的）内存块。[1, 2, 3... 10000]。所有数据必须同时存在于物理内存（RAM）中。
            CPU： 访问极快（O(1)），因为通过索引可以直接计算出内存地址。
        Iterator (状态机结构):
            内存： 它不保存数据集合，它只保存生成数据的逻辑和当前进度的状态。
            例如 range(100000000)。它在内存中只占几十个字节。它内部只存了三个变量：start=0, step=1, current_value=...。
            CPU 指令流： 当你调用 next() 时，CPU 执行一段代码：current_value += step，然后把结果返回给你，并暂停（Suspend）在这里，等待下一次调用。
        操作系统层面的 I/O（以文件迭代为例）：
            当你用 for line in open("big_file.txt") 时：
            1. 用户态： Python 迭代器请求 next。
            2. 内核态： 触发系统调用（如 Linux 的 read）。内核查看文件系统缓存（Page Cache）。
            3. 硬件： 如果缓存没命中，磁盘驱动器转动，读取物理扇区的数据。
            4. 数据流转： 数据被拷贝到用户空间的缓冲区 -> 转换成 Python 的 String 对象 -> 返回给你的代码。
            5. 关键点： 之前的 String 对象如果没有引用，会被 Python 的垃圾回收机制（GC）立刻回收。这就是为什么内存不涨的原因。
3. 思维陷阱：反直觉思考题
作为面试官，我讲了这么多它的好（省内存、惰性计算），现在我要给你挖个坑。请仔细思考下面这个问题，不要急着回答“是”或“否”，试着分析原因。
问题：
    既然迭代器极大地节省了内存，甚至在某些情况下避免了 I/O 阻塞，那为什么 Python 不把所有的列表（List）都默认设计成迭代器？
    或者说，在什么场景下，使用迭代器反而会比使用列表性能更差（更慢），甚至导致逻辑错误？

（提示：从 CPU 分支预测、数据访问模式、或者“一次性”的特点去考虑。）
"""

# 第二步：最小化实战验证（The "MVP" Code）
"""
现在，我们要把刚才吹的牛（省内存、底层协议）用代码落实。我们将手写一个简单的迭代器，并用工具“抓住”它的内存优势和“一次性”的弱点。
任务： 我们模拟一个生成巨大数字序列的场景。
1. 代码验证（MVP 版本）
请在你的 Python 环境中运行以下代码（或者在脑海中跑一遍）：
import sys
import time

# === 选手 A：传统的列表（Eager） ===
class EagerList:
    def __init__(self, n):
        self.data = [i for i in range(n)] # 瞬间生成所有数据占满内存

# === 选手 B：手写迭代器（Lazy） ===
# 这是理解迭代器底层原理的最简 MVP
class LazyIterator:
    def __init__(self, n):
        self.n = n
        self.current = 0  # 状态机：只记录当前游标
    
    def __iter__(self):
        return self  # 协议第一步：返回自己
    
    def __next__(self):  # 协议第二步：生产逻辑
        if self.current < self.n:
            num = self.current
            self.current += 1 # 更新状态
            return num
        else:
            raise StopIteration # 信号：没货了，别拧了

# === 实验配置 ===
N = 10_000_000  # 一千万数据

print(f"--- 实验开始：数据量 {N} ---")

# 1. 内存占用对比
# 注意：这里我们只通过创建对象来观察初始开销

# 测试列表
start_time = time.time()
try:
    # 警告：如果你电脑内存小，这个可能会卡死，建议先用小一点的数试
    list_obj = EagerList(N) 
    print(f"[List] 创建耗时: {time.time() - start_time:.4f}s")
    # sys.getsizeof 只能看外壳大小，对于 list 包含的内容大约需要 N * 8 字节 + 指针开销
    # 粗略估算：1000万个整数大约占用几百 MB
    print(f"[List] 内存估算: 约 {sys.getsizeof(list_obj.data) / 1024 / 1024:.2f} MB")
except MemoryError:
    print("[List] 内存爆炸了！")

# 测试迭代器
start_time = time.time()
iter_obj = LazyIterator(N)
print(f"[Iterator] 创建耗时: {time.time() - start_time:.4f}s")
print(f"[Iterator] 内存占用: {sys.getsizeof(iter_obj)} Bytes (没错，是字节！)")

# 2. 验证“一次性”缺陷
print("\n--- 验证陷阱：数据去哪了？ ---")
small_iter = LazyIterator(3) # 建个小的方便看

print("第一次遍历:")
for i in small_iter:
    print(i, end=' ') # 输出: 0 1 2

print("\n第二次遍历:")
for i in small_iter:
    print(i, end=' ') # 输出: (什么都没有！)
print("\n(看到了吗？第二次遍历是空的)")

2. 现象解析（结合第一步理论）
内存对比（空间）：
List: 几百 MB。因为它真的在堆内存里开了几千万个小房间放整数。
Iterator: 48 Bytes (或者类似的很小数值)。不管 N 是 100 还是 100 亿，它的大小不变。因为它只存了 n 和 current 两个整数变量。这就是用时间换空间。
创建速度（时间）：
List: 慢。因为要执行一千万次内存分配和赋值。
Iterator: 瞬间完成。因为它只是初始化了两个变量，根本没开始干活。
一次性陷阱：
当你第二次循环 small_iter 时，__next__ 方法被调用。
但此时 self.current 已经是 3 了。
判断 if self.current < self.n 失败，直接抛出 StopIteration。
for 循环捕获异常，优雅退出。对于调用者来说，表现为“空列表”。
3. 异常推演（What if...）
假设我们在生产环境使用这个 LazyIterator，请思考以下极端情况：
场景： 你的代码正在遍历一个基于网络的迭代器（比如从 Kafka 拉取数据），处理到一半时，你的程序崩溃重启了，或者网络断开了。
推演：
对于 List: 数据都在内存，断电即失。但如果是从磁盘读出来的，重读一遍就行。
对于 Iterator: 状态保存在哪里？
如果你没有外部机制记录“我读到第几个了”（Offset），当你重启程序，再次创建一个新的迭代器，你会从头开始读（数据重复处理），还是说如果源头数据流是实时的，刚才没处理完的数据就彻底丢失了？
思考题（进入第三步的门票）：
上面的 LazyIterator 是线程安全的吗？如果我有两个线程同时对同一个 iter_obj 进行 next() 操作，会发生什么？（结合 self.current += 1 这一行思考）。
请确认代码已看懂，并给出你对“多线程竞争”的看法。
"""
